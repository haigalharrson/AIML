{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Twitter_Disaster.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XHaL5si3o92"
      },
      "source": [
        "**Twitter Disaster Prediction** <br>\n",
        "\n",
        "Submitted by <br>\n",
        "* Haigal Harrison - C0816642\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZGOuQcy4L41"
      },
      "source": [
        "roBERTa model from tensorflow is used to predict whether a tweet is about a real disaster or not"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMYZ8QxdHOvf"
      },
      "source": [
        "# Importing the required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ge0RJUSWH8ZC",
        "outputId": "4962822f-8837-45c8-91e9-821cc855a45d"
      },
      "source": [
        "# Mounting Drive\n",
        "drive.mount('/content/drive/',force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfwhx8ovIC0U"
      },
      "source": [
        "# importing the required tensorflow libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import tensorflow_hub as hub"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9htv6Mk_ITsV"
      },
      "source": [
        "!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3XCwD1wIopj",
        "outputId": "000a884a-fd4d-4f20-a301-3622e0b54256"
      },
      "source": [
        "!pip install sentencepiece"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[?25l\r\u001b[K     |▎                               | 10 kB 27.8 MB/s eta 0:00:01\r\u001b[K     |▌                               | 20 kB 31.3 MB/s eta 0:00:01\r\u001b[K     |▉                               | 30 kB 12.1 MB/s eta 0:00:01\r\u001b[K     |█                               | 40 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |█▍                              | 51 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█▋                              | 61 kB 5.9 MB/s eta 0:00:01\r\u001b[K     |██                              | 71 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 81 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 92 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 102 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███                             | 112 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 122 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 133 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 143 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████                            | 153 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 163 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 174 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 184 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 194 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 204 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 215 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████                          | 225 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 235 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 245 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 256 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████                         | 266 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 276 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 286 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 296 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████                        | 307 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 317 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 327 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 337 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 348 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 358 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 368 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 378 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 389 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 399 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 409 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 419 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 430 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 440 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 450 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 460 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 471 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 481 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 491 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 501 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 512 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 522 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 532 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 542 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 552 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 563 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 573 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 583 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 593 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 604 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 614 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 624 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 634 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 645 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 655 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 665 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 675 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 686 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 696 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 706 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 716 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 727 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 737 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 747 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 757 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 768 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 778 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 788 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 798 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 808 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 819 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 829 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 839 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 849 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 860 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 870 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 880 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 890 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 901 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 911 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 921 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 931 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 942 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 952 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 962 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 972 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 983 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 993 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.0 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.0 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.0 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.0 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.0 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.1 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.1 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.1 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.1 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.1 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.1 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.1 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.1 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.1 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.1 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.2 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.2 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.2 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.2 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.2 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.2 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.2 MB 5.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSEsow55IZ-W"
      },
      "source": [
        "import tokenization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pxR1os3IcGL"
      },
      "source": [
        "# a seed is set of better reproducability\n",
        "SEED = 1002\n",
        "def seed_everything(seed):\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed) \n",
        "    \n",
        "seed_everything(SEED)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "WXKQhx6CIvtj",
        "outputId": "065b9d58-762d-4739-9c70-c8da552b0ed2"
      },
      "source": [
        "# reading the training data from the google drive using pandas\n",
        "train_df = pd.read_csv('/content/drive/My Drive/My data files/twitter_disaster_data.csv')\n",
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>All residents asked to 'shelter in place' are ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id keyword  ...                                               text target\n",
              "0   1     NaN  ...  Our Deeds are the Reason of this #earthquake M...      1\n",
              "1   4     NaN  ...             Forest fire near La Ronge Sask. Canada      1\n",
              "2   5     NaN  ...  All residents asked to 'shelter in place' are ...      1\n",
              "3   6     NaN  ...  13,000 people receive #wildfires evacuation or...      1\n",
              "4   7     NaN  ...  Just got sent this photo from Ruby #Alaska as ...      1\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Nzuxq8e502t"
      },
      "source": [
        "Bert tokenization method is used to tokenize the tweets collected via twitter API. It includes the following 3 steps,\n",
        "\n",
        "* Text Normalization - The text is converted to lowercase , all the whitespaces are converted to spaces and get rid of accent markers\n",
        "\n",
        "* Punctuation Splitting - A space is added to each side of the punctuation marks\n",
        "\n",
        "* It is also called whitespace tokenization and applies WordPiece tokenization to every word\n",
        "\n",
        "Finally it reduces the length of the text to the given length we have provided and add [CLS] and [SEP] tags to the end. [CLS] indicates the start and [SEP] indicates the end of each sentence. <br>\n",
        "Tokenizer method is used to convert words to integers. An input mask and segments ids are also created."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EuvkYJLXJGEA"
      },
      "source": [
        "def encode_bert(txt, tokenizer, max_length=512):\n",
        "  tokens = []\n",
        "  masks = []\n",
        "  segments = []\n",
        "  for t in txt:\n",
        "    t = tokenizer.tokenize(t)\n",
        "    t = t[:max_length-2]\n",
        "    inp_seq = [\"[CLS]\"] + t + [\"[SEP]\"]\n",
        "    pad_length = max_length - len(inp_seq)\n",
        "\n",
        "    token = tokenizer.convert_tokens_to_ids(inp_seq)\n",
        "    token += [0] * pad_length\n",
        "    pad_masks = [1] * len(inp_seq) + [0] * pad_length\n",
        "    seg_ids = [0] * max_length\n",
        "\n",
        "    tokens.append(token)\n",
        "    masks.append(pad_masks)\n",
        "    segments.append(seg_ids)\n",
        "    \n",
        "  return np.array(tokens), np.array(masks), np.array(segments)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zx2hp_Nh86h_"
      },
      "source": [
        "Plain text input is converted to the expected input format by the roBERTa Model. <br>\n",
        "\n",
        "* input_word_ids : Maps each words to its corresponding token id.\n",
        "* input_mask : Specified the start and end of a sentence using an array. No padding tokens are given a value of 1 and padding tokens are given a value of 0.\n",
        "* segment_ids : Recognizes the segments of the sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRMii30tLEbJ"
      },
      "source": [
        "def model_build(bert_layer, max_length=512):\n",
        "  inp_word_ids = Input(shape=(max_length,), dtype=tf.int32, name=\"inp_word_ids\")\n",
        "  inp_mask = Input(shape=(max_length,), dtype=tf.int32, name=\"inp_mask\")\n",
        "  seg_ids = Input(shape=(max_length,), dtype=tf.int32, name=\"seg_ids\")\n",
        "\n",
        "  _, sequence_output = bert_layer([inp_word_ids, inp_mask, seg_ids])\n",
        "  clf_output = sequence_output[:, 0, :]\n",
        "  out = Dense(1, activation='sigmoid')(clf_output)\n",
        "\n",
        "  model = Model(inputs=[inp_word_ids, inp_mask, seg_ids], outputs=out)\n",
        "    \n",
        "  model.compile(Adam(learning_rate=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    \n",
        "  return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDDgRwkb-utM"
      },
      "source": [
        "Loads the roBERTa model. Large uncased model is used. It is set as a Keras Layer to use it to our specific case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzAd8utNMDN7"
      },
      "source": [
        "module_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\"\n",
        "bert_layer = hub.KerasLayer(module_url, trainable=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fW2UXvn_WJB"
      },
      "source": [
        "Tokenizer converts the input data to a format BERT understands. Vocab file is used so that tokenizer knows which number to be used to encode each words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYcp4OCwMHf9"
      },
      "source": [
        "voc_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "\n",
        "#returns true/false based on selected cased/uncased bert layer\n",
        "lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
        "\n",
        "#Initialize the tokenizer\n",
        "tokenizer = tokenization.FullTokenizer(voc_file, lower_case)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgGqVoB6NEKx"
      },
      "source": [
        "train_inp = encode_bert(train_df.text.values, tokenizer, max_length=160)\n",
        "train_label = train_df.target.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSR8srAiAbv9"
      },
      "source": [
        "The model summary shows three input layers we created followed by roBERTa model in the Keras layer. Last dense layer predicts the output in a scale of 0-1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTHAQirbNian",
        "outputId": "bf447309-452f-4810-aebc-0a5842bfc16d"
      },
      "source": [
        "model = model_build(bert_layer, max_length=160)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " inp_word_ids (InputLayer)      [(None, 160)]        0           []                               \n",
            "                                                                                                  \n",
            " inp_mask (InputLayer)          [(None, 160)]        0           []                               \n",
            "                                                                                                  \n",
            " seg_ids (InputLayer)           [(None, 160)]        0           []                               \n",
            "                                                                                                  \n",
            " keras_layer (KerasLayer)       [(None, 1024),       335141889   ['inp_word_ids[0][0]',           \n",
            "                                 (None, 160, 1024)]               'inp_mask[0][0]',               \n",
            "                                                                  'seg_ids[0][0]']                \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 1024)        0           ['keras_layer[0][1]']            \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 1)            1025        ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 335,142,914\n",
            "Trainable params: 335,142,913\n",
            "Non-trainable params: 1\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MCKpgFPA7ge"
      },
      "source": [
        "Model is trained using Keras. ModelCheckpoint is used to save the best model with high validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nSQ1ruLSNtde",
        "outputId": "ec4d5533-ad53-4cb4-b625-7dac74f44624"
      },
      "source": [
        "model_checkpoint = ModelCheckpoint('model.h5', monitor='val_accuracy', save_best_only=True)\n",
        "history = model.fit(\n",
        "    train_inp, train_label,\n",
        "    validation_split=0.1,\n",
        "    epochs=3,\n",
        "    callbacks=[model_checkpoint],\n",
        "    batch_size=5\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "1371/1371 [==============================] - 1667s 1s/step - loss: 0.4232 - accuracy: 0.8234 - val_loss: 0.3645 - val_accuracy: 0.8425\n",
            "Epoch 2/3\n",
            "1371/1371 [==============================] - 1621s 1s/step - loss: 0.2655 - accuracy: 0.8981 - val_loss: 0.4105 - val_accuracy: 0.8504\n",
            "Epoch 3/3\n",
            "1371/1371 [==============================] - 1556s 1s/step - loss: 0.1586 - accuracy: 0.9399 - val_loss: 0.5257 - val_accuracy: 0.8360\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CHaQ4SNBSoe"
      },
      "source": [
        "Model is saved to google drive so that it can be used to predict tweets in realtime."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4Gd4dsHqrJx",
        "outputId": "412e2dbc-8828-4ac8-d8c9-44b9911989f7"
      },
      "source": [
        "model.save('/content/drive/My Drive/My data files/twitter_model')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 1595). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/My data files/twitter_model/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/My data files/twitter_model/assets\n"
          ]
        }
      ]
    }
  ]
}
